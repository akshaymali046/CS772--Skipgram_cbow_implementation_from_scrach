{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0acX9ogGXzLb",
        "outputId": "02b67063-2f3c-44b1-938b-02e0fe142532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from wikipedia-api) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (2.10)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.5.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "!pip3 install wikipedia-api\n",
        "import wikipediaapi\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "import numpy as np\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_data=list(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "with open(\"/content/Analogy_dataset.txt\", \"r\") as f:\n",
        "    lines = f.read().splitlines()\n",
        "analogies=[]\n",
        "for i in range(len(lines)):\n",
        "    analogies +=lines[i].split(\" \")\n",
        "analogies=analogies[:-1]"
      ],
      "metadata": {
        "id": "C45DSzBiX4MR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_k_sentences(word,k):\n",
        "    wiki_wiki = wikipediaapi.Wikipedia(\n",
        "        language='en',\n",
        "        extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "\n",
        "    p_wiki = wiki_wiki.page(word)\n",
        "    txt=p_wiki.text\n",
        "    sentences = txt.split(\".\")\n",
        "    k_sentences = []\n",
        "    for sent in sentences:\n",
        "        check_word = sent.split(\" \")\n",
        "        if word in check_word and len(check_word) < 50:\n",
        "            k_sentences.append(sent)\n",
        "        if len(k_sentences) > k:\n",
        "            break\n",
        "    return k_sentences"
      ],
      "metadata": {
        "id": "KNwJQZuPZOOp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenised_additional_corpus(k):\n",
        "    analogy_raw_corpus = []\n",
        "    for analogy in analogies:\n",
        "      if analogy != \" \":\n",
        "          analogy_raw_corpus += get_k_sentences(analogy,k)\n",
        "    full_analogy_corpus = ' '.join([elem for elem in analogy_raw_corpus])\n",
        "    data = re.sub(r'[,!?;-]+\\n:', '.', full_analogy_corpus)\n",
        "    data = nltk.word_tokenize(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "pX1VdQiaZS8f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_corpus(data,stemmer):\n",
        "    corpus=data\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    pattern = re.compile(\"^[a-zA-Z]+$\")\n",
        "    porter = PorterStemmer()\n",
        "    data=[]\n",
        "    for ch in corpus:\n",
        "        ch = ch.lower()\n",
        "        if pattern.match(ch) and ch not in stop_words and ch != '.':\n",
        "                if stemmer==True:\n",
        "                     data.append(porter.stem(ch))\n",
        "                else:\n",
        "                     data.append(ch)\n",
        "    return data"
      ],
      "metadata": {
        "id": "6t1Zw7CLZVSf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get full corpus data (after performing pre processing) = gutenburg corpus + k tokenized sentences"
      ],
      "metadata": {
        "id": "6dn5ab-tkzDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_corpus = full_corpus(tokenised_additional_corpus(5),False)+full_corpus(corpus_data,True) #initailly assuming k=6 to check"
      ],
      "metadata": {
        "id": "-KXt8jtDZYxP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covab_corpus=list(sorted(set(processed_corpus)))"
      ],
      "metadata": {
        "id": "NGwNLNCH57NY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(covab_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2Qfuglq6IIV",
        "outputId": "4864a79e-a067-4b7f-bd27-6fe2ae8a8cbb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6698"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist = nltk.FreqDist(word for word in processed_corpus)\n",
        "print(\"Total words in corpus: \",len(processed_corpus) )\n",
        "print(\"Size of vocabulary: \",len(freq_dist) )\n",
        "print(\"Most frequent tokens: \",freq_dist.most_common(20) )\n",
        "V=len(freq_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9txhaf6lYcd",
        "outputId": "9030fde1-3c80-47bd-c383-5aa97a725ef5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in corpus:  85757\n",
            "Size of vocabulary:  6698\n",
            "Most frequent tokens:  [('mr', 1852), ('emma', 865), ('could', 838), ('would', 826), ('miss', 611), ('must', 568), ('one', 516), ('harriet', 506), ('much', 494), ('said', 487), ('think', 466), ('thing', 460), ('weston', 448), ('everi', 435), ('well', 415), ('elton', 407), ('knightley', 396), ('know', 393), ('say', 392), ('good', 365)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def indices_info(processed_corpus):\n",
        "    processed = sorted(list(set(processed_corpus)))\n",
        "    index = 0\n",
        "    wordxInd = {}\n",
        "    Indxword = {}\n",
        "    for word in processed:\n",
        "        if word not in wordxInd: \n",
        "             wordxInd[word] = index\n",
        "             Indxword[index] = word\n",
        "             index += 1\n",
        "    return wordxInd, Indxword"
      ],
      "metadata": {
        "id": "Rw6E_WnEvG_V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covab_corpus[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlVrtv5U669n",
        "outputId": "567dd7fd-5a54-4fd7-9daf-ee2e5abfe058"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abbey', 'abbot', 'abdi', 'abdul', 'abhor']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "wordxInd, Indxword = indices_info(processed_corpus)\n",
        "print(dict(itertools.islice(wordxInd.items(), 5)))\n",
        "print(dict(itertools.islice(Indxword.items(), 5)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_auBB_cErvt1",
        "outputId": "aef7f137-659e-4b49-ba71-c09ade3448d1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'abbey': 0, 'abbot': 1, 'abdi': 2, 'abdul': 3, 'abhor': 4}\n",
            "{0: 'abbey', 1: 'abbot', 2: 'abdi', 3: 'abdul', 4: 'abhor'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary =covab_corpus\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'vocab': vocabulary})\n",
        "one_hot_encoded = pd.get_dummies(df['vocab'])"
      ],
      "metadata": {
        "id": "Ljp8l_TH2-AO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "zWkk8LOC72S3",
        "outputId": "b9b48481-0538-4c4e-9016-689ad0b443f4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      abbey  abbot  abdi  abdul  abhor  abid  abil  abjad  abl  abod  ...  \\\n",
              "0         1      0     0      0      0     0     0      0    0     0  ...   \n",
              "1         0      1     0      0      0     0     0      0    0     0  ...   \n",
              "2         0      0     1      0      0     0     0      0    0     0  ...   \n",
              "3         0      0     0      1      0     0     0      0    0     0  ...   \n",
              "4         0      0     0      0      1     0     0      0    0     0  ...   \n",
              "...     ...    ...   ...    ...    ...   ...   ...    ...  ...   ...  ...   \n",
              "6693      0      0     0      0      0     0     0      0    0     0  ...   \n",
              "6694      0      0     0      0      0     0     0      0    0     0  ...   \n",
              "6695      0      0     0      0      0     0     0      0    0     0  ...   \n",
              "6696      0      0     0      0      0     0     0      0    0     0  ...   \n",
              "6697      0      0     0      0      0     0     0      0    0     0  ...   \n",
              "\n",
              "      youth  yugoslavia  zamora  zeal  zealand  zigzag  zimbabwe  zone  zones  \\\n",
              "0         0           0       0     0        0       0         0     0      0   \n",
              "1         0           0       0     0        0       0         0     0      0   \n",
              "2         0           0       0     0        0       0         0     0      0   \n",
              "3         0           0       0     0        0       0         0     0      0   \n",
              "4         0           0       0     0        0       0         0     0      0   \n",
              "...     ...         ...     ...   ...      ...     ...       ...   ...    ...   \n",
              "6693      0           0       0     0        0       1         0     0      0   \n",
              "6694      0           0       0     0        0       0         1     0      0   \n",
              "6695      0           0       0     0        0       0         0     1      0   \n",
              "6696      0           0       0     0        0       0         0     0      1   \n",
              "6697      0           0       0     0        0       0         0     0      0   \n",
              "\n",
              "      zoroastrianism  \n",
              "0                  0  \n",
              "1                  0  \n",
              "2                  0  \n",
              "3                  0  \n",
              "4                  0  \n",
              "...              ...  \n",
              "6693               0  \n",
              "6694               0  \n",
              "6695               0  \n",
              "6696               0  \n",
              "6697               1  \n",
              "\n",
              "[6698 rows x 6698 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-507a4336-4a69-4749-8e02-ccc82b96027c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abbey</th>\n",
              "      <th>abbot</th>\n",
              "      <th>abdi</th>\n",
              "      <th>abdul</th>\n",
              "      <th>abhor</th>\n",
              "      <th>abid</th>\n",
              "      <th>abil</th>\n",
              "      <th>abjad</th>\n",
              "      <th>abl</th>\n",
              "      <th>abod</th>\n",
              "      <th>...</th>\n",
              "      <th>youth</th>\n",
              "      <th>yugoslavia</th>\n",
              "      <th>zamora</th>\n",
              "      <th>zeal</th>\n",
              "      <th>zealand</th>\n",
              "      <th>zigzag</th>\n",
              "      <th>zimbabwe</th>\n",
              "      <th>zone</th>\n",
              "      <th>zones</th>\n",
              "      <th>zoroastrianism</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6693</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6694</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6695</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6696</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6697</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6698 rows × 6698 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-507a4336-4a69-4749-8e02-ccc82b96027c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-507a4336-4a69-4749-8e02-ccc82b96027c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-507a4336-4a69-4749-8e02-ccc82b96027c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_windows(words, C):\n",
        "    i = C\n",
        "    while i < len(words) - C:\n",
        "        center_word = words[i]\n",
        "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
        "        yield context_words, center_word\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "M1qzxIIid1GP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in get_windows(processed_corpus[2000:2010],3):\n",
        "    print(f'{x}\\t{y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE-8kVmzeNN3",
        "outputId": "c4b56a3e-7d7f-4989-85cc-0a82a7235081"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['people', 'also', 'part', 'urban', 'agglomeration', 'africa']\tlargest\n",
            "['also', 'part', 'largest', 'agglomeration', 'africa', 'arab']\turban\n",
            "['part', 'largest', 'urban', 'africa', 'arab', 'world']\tagglomeration\n",
            "['largest', 'urban', 'agglomeration', 'arab', 'world', 'middle']\tafrica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoded['though'].to_numpy().reshape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKj0CnvtS3sD",
        "outputId": "75a588c5-2135-4fa1-bc6b-9229a57f994f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function ndarray.reshape>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_set():\n",
        "   train_x =None\n",
        "   train_y = None\n",
        "   \n",
        "   for context_words, center_word in get_windows(processed_corpus[0:5000], 3):\n",
        "      center_word_vector=one_hot_encoded[center_word].to_numpy()\n",
        "      center_word_vector=np.resize(center_word_vector,(center_word_vector.shape[0],1))\n",
        "      context_words_vector=0  \n",
        "      for word in context_words:      \n",
        "           context_words_vector += one_hot_encoded[word].to_numpy()\n",
        "      context_words_vector=np.resize(context_words_vector,(context_words_vector.shape[0],1))\n",
        "      if train_x is not None:\n",
        "           train_x=np.append(train_x, context_words_vector/len(context_words), axis=1)\n",
        "           train_y=np.append(train_y, center_word_vector, axis=1)\n",
        "      else:\n",
        "           train_x=context_words_vector/len(context_words)\n",
        "           train_y=center_word_vector\n",
        "\n",
        "   return train_x,train_y"
      ],
      "metadata": {
        "id": "Qd01-XNL1v13"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y= get_train_set()\n"
      ],
      "metadata": {
        "id": "HvyjBtrEV17h"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLJdW93Y7s8Y",
        "outputId": "fb5626cd-590e-4c49-8cc0-d55168e7a01c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(N,V, random_seed=1):\n",
        "    np.random.seed(random_seed)\n",
        "    W1 = np.random.rand(N,V)\n",
        "    W2 = np.random.rand(V,N)\n",
        "    b1 = np.random.rand(N,1)\n",
        "    b2 = np.random.rand(V,1)\n",
        "    return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "0K4TP5McbCRJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    e_z = np.exp(z)\n",
        "    sum_e_z = np.sum(e_z,axis=0,keepdims=True)\n",
        "    return e_z/sum_e_z"
      ],
      "metadata": {
        "id": "EPhYc3QEbCUh"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(x, W1, W2, b1, b2):\n",
        "    h = np.dot(W1,x) + b1\n",
        "    h = np.maximum(0,h)\n",
        "    z = np.dot(W2,h) + b2\n",
        "    return z, h"
      ],
      "metadata": {
        "id": "PAiOUrNgbCXy"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(y, yhat):\n",
        "    m=y.shape[1]\n",
        "    logprobs = np.multiply(np.log(yhat),y)\n",
        "    cost = - 1/m * np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "R6CS4TIObCbQ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(x, yhat, y, h, W1, W2, b1, b2):\n",
        "    m=x.shape[1]\n",
        "    l1 = np.dot(W2.T ,(yhat - y))\n",
        "    l1 = np.maximum(0,l1)\n",
        "    grad_W1 = np.dot(l1,x.T)/m\n",
        "    grad_W2 = np.dot((yhat - y),h.T)/m\n",
        "    grad_b1 = np.sum(l1,axis=1,keepdims=True)/m\n",
        "    grad_b2 = np.sum((yhat - y),axis=1,keepdims=True)/m\n",
        "    return grad_W1, grad_W2, grad_b1, grad_b2"
      ],
      "metadata": {
        "id": "GtxsVcpZrV3m"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x, y = get_train_set()"
      ],
      "metadata": {
        "id": "a-frEmT0BEmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(N, num_iters,alpha, \n",
        "                     random_seed, initialize_model=initialize_model, \n",
        "                     forward_prop=forward_prop, \n",
        "                     softmax=softmax,\n",
        "                     compute_cost=compute_cost, \n",
        "                     back_prop=back_prop):\n",
        "  \n",
        "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=random_seed)\n",
        "    iters = 0\n",
        "    for i in range(num_iters+1):\n",
        "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
        "        yhat = softmax(z)\n",
        "        cost = compute_cost(y, yhat)\n",
        "        if ( (iters+1) % 10 == 0):\n",
        "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2)\n",
        "\n",
        "        W1 = W1 - alpha * grad_W1\n",
        "        W2 = W2 - alpha * grad_W2\n",
        "        b1 = b1 - alpha * grad_b1\n",
        "        b2 = b2 - alpha * grad_b2\n",
        "        iters +=1 \n",
        "        if iters == num_iters: \n",
        "            break\n",
        "            \n",
        "    return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "HrtBz1uy6RRf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1, W2, b1, b2=gradient_descent(500, 210,0.06,1)"
      ],
      "metadata": {
        "id": "AC2EfcwG6jbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9b1332e-6b84-4553-ebb3-4994d8298c54"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iters: 10 cost: 17.749115\n",
            "iters: 20 cost: 15.501379\n",
            "iters: 30 cost: 13.941503\n",
            "iters: 40 cost: 12.746420\n",
            "iters: 50 cost: 11.804557\n",
            "iters: 60 cost: 11.058228\n",
            "iters: 70 cost: 10.458509\n",
            "iters: 80 cost: 9.969619\n",
            "iters: 90 cost: 9.572253\n",
            "iters: 100 cost: 9.253129\n",
            "iters: 110 cost: 9.007025\n",
            "iters: 120 cost: 8.827385\n",
            "iters: 130 cost: 8.710967\n",
            "iters: 140 cost: 8.654843\n",
            "iters: 150 cost: 8.646518\n",
            "iters: 160 cost: 8.671101\n",
            "iters: 170 cost: 8.713344\n",
            "iters: 180 cost: 8.758393\n",
            "iters: 190 cost: 8.794323\n",
            "iters: 200 cost: 8.817982\n",
            "iters: 210 cost: 8.830083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLKdzLxCLyPU",
        "outputId": "6a967af7-91ee-47ed-df06-a04aef1e7094"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04, ...,\n",
              "        2.85312388e-01, 8.71852190e-01, 4.71602282e-02],\n",
              "       [3.64310657e-01, 7.74035414e-01, 4.85513882e-01, ...,\n",
              "        1.79367285e-01, 8.80412834e-01, 2.19778595e-01],\n",
              "       [5.17081475e-01, 6.80846766e-01, 3.29858347e-01, ...,\n",
              "        6.04752846e-01, 5.46033231e-01, 9.42977127e-01],\n",
              "       ...,\n",
              "       [4.81957834e-01, 4.71676473e-01, 8.80020169e-01, ...,\n",
              "        5.14653681e-01, 7.18061055e-01, 6.73145456e-01],\n",
              "       [2.01923343e-01, 6.53384118e-01, 4.41259750e-01, ...,\n",
              "        2.86217691e-01, 8.05183709e-01, 5.93183564e-01],\n",
              "       [1.75523836e-01, 8.13210086e-01, 6.87718710e-01, ...,\n",
              "        9.83985406e-01, 7.65344714e-01, 9.70616300e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean(A, B):\n",
        "   \n",
        "    d = np.linalg.norm(A-B)\n",
        "\n",
        "\n",
        "    return d"
      ],
      "metadata": {
        "id": "8jC74uXWOStk"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(A, B):\n",
        " \n",
        "\n",
        "    dot = np.dot(A,B)  \n",
        "    norma = np.linalg.norm(A)\n",
        "    normb = np.linalg.norm(B)\n",
        "    cos = dot/(norma*normb)\n",
        "\n",
        "    return cos"
      ],
      "metadata": {
        "id": "aS8PadcvOnf1"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_analogy(word1, word2, word3, embeddings, cosine_similarity=cosine_similarity):\n",
        "\n",
        "    group=[word1.lower(), word2.lower(), word3.lower()]\n",
        "    word1_emb = embeddings[wordxInd[word1.lower()]]\n",
        "    word2_emb = embeddings[wordxInd[word2.lower()]]\n",
        "    word3_emb = embeddings[wordxInd[word3.lower()]]\n",
        "\n",
        "    # Remember: King - Man + Woman = Queen\n",
        "    vec =  word2_emb - word1_emb + word3_emb\n",
        "\n",
        "    similarity = -1\n",
        "\n",
        "    country = ''\n",
        "\n",
        "    for word in covab_corpus:\n",
        "\n",
        "        if word not in group :\n",
        "\n",
        "            # get the word embedding\n",
        "            word_emb = embeddings[wordxInd[word.lower()]]\n",
        "\n",
        "            cur_similarity = cosine_similarity(word_emb,vec)\n",
        "\n",
        "            if cur_similarity > similarity:\n",
        "\n",
        "                similarity = cur_similarity\n",
        "\n",
        "                country = (word,similarity)\n",
        "\n",
        "    return country"
      ],
      "metadata": {
        "id": "wj7n8_uYOuJj"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embs = (W1.T+W2)/2"
      ],
      "metadata": {
        "id": "yvW5hGgsP3p7"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_analogy(\"Maharastra\",\"Mumbai\", \"Karnataka\", embs, cosine_similarity=cosine_similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgLvY5zdPid-",
        "outputId": "dea9caa6-e513-473d-c5cf-ecfe7a4eef73"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('cosmopolitan', 0.7657802968477159)"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvDjtk5IXSE8",
        "outputId": "4ef2adb6-4b1f-4a9a-b1e3-2ac99cd17076"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6698, 4994)"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    }
  ]
}