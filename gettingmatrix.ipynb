{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKKPbllzFblA",
    "outputId": "dbe751d1-32f3-4baa-9ca4-5acdd1b4cd1e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m3ZNtE75FhAZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_corpus(words):\n",
    "   stop_words = set(stopwords.words('english'))\n",
    "   pattern = re.compile(\"^[a-zA-Z]+$\")\n",
    "   readycorpus=[]\n",
    "   for ch in words:\n",
    "       ch = ch.lower()\n",
    "       if pattern.match(ch) and ch not in stop_words and ch != '.' and ch !=\" \":\n",
    "            readycorpus.append(ch)\n",
    "   return readycorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yo-dtTZeFwG4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "file1 = open(\"blah.txt\",\"r\",encoding=\"utf8\")\n",
    "wikicorpous=file1.read()\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ViSBRTauHECQ",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e543fc01a584d64b0cd7be08b6d363d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98552 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus = gutenberg.sents()\n",
    "gutenbergsentlist=list(corpus)\n",
    "guten_window_corpus=[]\n",
    "fullgutencorpus=[]\n",
    "for sent in tqdm(gutenbergsentlist):\n",
    "    cleaned_raw= clean_corpus(sent)\n",
    "    if len(cleaned_raw)>=3:\n",
    "        fullgutencorpus+=cleaned_raw\n",
    "        guten_window_corpus.append(cleaned_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T6MgrA18HsZm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "wikilist=wikicorpous.split(\".\")\n",
    "wikisentlist=[]\n",
    "for sent in wikilist:\n",
    "     wikisentlist.append(sent.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FQw22VQgJFxY",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a41045634249b495c650ec51d98bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_window_corpus=[]\n",
    "fullwikicorpus=[]\n",
    "for sent in tqdm(wikisentlist):\n",
    "    cleaned_raw= clean_corpus(sent)\n",
    "    if len(cleaned_raw)>=3:\n",
    "        fullwikicorpus+=cleaned_raw\n",
    "        wiki_window_corpus.append(cleaned_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "w0CURyPQJ0cw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fullcorpus=fullgutencorpus+fullwikicorpus\n",
    "windowfullcorpus=guten_window_corpus+wiki_window_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rMwp66lbKpc4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary=49245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aadmi', 'aam', 'aaron', 'aaronites', 'aarthik']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary=list(sorted(set(fullcorpus)))\n",
    "print(f\"vocabulary={len(vocabulary)}\")\n",
    "vocabulary[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "y58v6jH-Kst4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "wordxInd = {}\n",
    "Indxword = {}\n",
    "for word in vocabulary:\n",
    "    if word not in wordxInd: \n",
    "        wordxInd[word] = index\n",
    "        Indxword[index] = word\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(corpus, C):\n",
    "    words=corpus\n",
    "    i=0\n",
    "    while i < len(words):\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cop=[]\n",
    "for li in windowfullcorpus:\n",
    "    cop+=li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch training samples : 1209738\n"
     ]
    }
   ],
   "source": [
    "leng=0\n",
    "for context,center in get_windows(cop,5):\n",
    "    leng+=1\n",
    "print(f\"batch training samples : {leng}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getxy_skipgram(V,leng,window_corpus,wordxInd,windowsize):\n",
    "    x_dumy=lil_matrix((V,leng))\n",
    "    y_dumy=lil_matrix((V,leng))\n",
    "    for i,words in tqdm(enumerate(get_windows(window_corpus,windowsize))):\n",
    "        x_dumy[wordxInd[words[1]],i]=1\n",
    "        for word in words[0]:\n",
    "            y_dumy[wordxInd[word],i]=1\n",
    "    return x_dumy,y_dumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc69eb083c2f4b698e2d6dcb9a44590b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_dumy,y_dumy=getxy_skipgram(len(vocabulary),leng,cop,wordxInd,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "x_dumy=x_dumy.tocsr(copy=False)\n",
    "sp.save_npz('x_skip_win5.npz', x_dumy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "y_dumy=y_dumy.tocsr(copy=False)\n",
    "sp.save_npz('y_skip_win5.npz', y_dumy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getxy_cbow(V,leng,window_corpus,wordxInd,windowsize):\n",
    "  x_dumy=lil_matrix((V,leng))\n",
    "  y_dumy=lil_matrix((V,leng))\n",
    "  for i,words in enumerate(get_windows(window_corpus, windowsize)):\n",
    "    y_dumy[wordxInd[words[1]],i]=1\n",
    "    for word in words[0]:\n",
    "        x_dumy[wordxInd[word],i]=1\n",
    "  return x_dumy,y_dumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_dumy1,y_dumy1=getxy_cbow(len(vocabulary),leng,cop,wordxInd,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "x_dumy1=x_dumy1.tocsr(copy=False)\n",
    "sp.save_npz('x_cbow_win5.npz', x_dumy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "y_dumy1=y_dumy1.tocsr(copy=False)\n",
    "sp.save_npz('x_cbow_win5.npz', y_dumy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W0=np.random.uniform(low=-0.8, high=0.8, size=(V,dims))\n",
    "W1=np.random.uniform(low=-0.8, high=0.8, size=(dims,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_z = np.exp(x)\n",
    "    sum_exp_z = np.sum(exp_z,axis=0,keepdims=True)\n",
    "    softmax_probs = exp_z / sum_exp_z\n",
    "    return softmax_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward(W0,W1,x):\n",
    "    h=np.dot(W0.T,x)\n",
    "    v=np.dot(W1.T,h)\n",
    "    return h,softmax(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backprop(W0,W1,x,y_true,alpha):\n",
    "    h,y_pred=forward(W0,W1,x)\n",
    "    y_pred=np.maximum(1e-8,y_pred)\n",
    "    loss = - np.sum(y_true*np.log(y_pred))/y_true.shape[1]\n",
    "    l1 = np.dot(W1 ,(y_pred - y_true))\n",
    "    dW1 = np.dot(h,(y_pred - y_true).T)\n",
    "    dW0 = np.dot(x,l1.T)\n",
    "    W1 -= alpha * dW1\n",
    "    W0 -= alpha * dW0\n",
    "    return W0,W1,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minibatch(W0,W1,x,y,leng,epochs):\n",
    "    W0x,W1x=W0,W1\n",
    "    e=0\n",
    "    while e<=epochs:\n",
    "        info=\"\"\n",
    "        batchsize=100\n",
    "        i=0\n",
    "        while batchsize+i <= leng:    \n",
    "            W0x,W1x,loss=backprop(W0x,W1x,np.asarray(x[:,i:i+batchsize].toarray()),np.asarray(y[:,i:i+batchsize].toarray()),0.03)\n",
    "            if (i+batchsize)% 50000==0:\n",
    "                info+=f\"loss at epoch {e} with batch {i} - {i+batchsize} is {loss}\\n\"\n",
    "                print(f\"loss at epoch {e} with batch {i} - {i+batchsize} is {loss}\")\n",
    "          \n",
    "            if i+batchsize == leng:\n",
    "                info+=f\"returned to batch of epoch {e} with loss at {i} - {i+batchsize} is {loss}\\n\"\n",
    "                print(f\"returned to batch of epoch {e} with loss at {i} - {i+batchsize} is {loss}\")\n",
    "\n",
    "                if not np.isnan(W0x[1,2]):\n",
    "                    print(f\"content w0,w1 saved for epoch {e} at {i+batchsize}\")\n",
    "                    np.save(f'W0epoch{e}.npy',W0x)\n",
    "                    np.save(f'W1epoch{e}.npy',W1x)\n",
    "                    with open('loss.txt', 'a') as file:\n",
    "                          file.write(info)\n",
    "\n",
    "            if i+batchsize == 659400:\n",
    "              batchsize=(leng-(batchsize+i))\n",
    "          \n",
    "            i+=100\n",
    "      \n",
    "        e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "leng=x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-TDixj5ZUDD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "minibatch(W0,W1,x,y,leng,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
