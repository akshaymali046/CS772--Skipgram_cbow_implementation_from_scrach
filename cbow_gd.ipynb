{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0acX9ogGXzLb",
        "outputId": "ab416707-63a1-40bd-ef52-3f095b79792b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from wikipedia-api) (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->wikipedia-api) (4.0.0)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.5.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "!pip3 install wikipedia-api\n",
        "import wikipediaapi\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "import numpy as np\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allbooks=list(gutenberg.fileids())\n",
        "corpus_data=[]\n",
        "for i in range(len(allbooks)):\n",
        "    corpus_data += list(gutenberg.words(allbooks[i]))\n",
        "with open(\"/content/Analogy_dataset.txt\", \"r\") as f:\n",
        "    lines = f.read().splitlines()\n",
        "analogies=[]\n",
        "for i in range(len(lines)):\n",
        "    analogies +=lines[i].split(\" \")\n",
        "analogies=analogies[:-1]"
      ],
      "metadata": {
        "id": "C45DSzBiX4MR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_k_sentences(word,k):\n",
        "    wiki_wiki = wikipediaapi.Wikipedia(\n",
        "        language='en',\n",
        "        extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
        "\n",
        "    p_wiki = wiki_wiki.page(word)\n",
        "    txt=p_wiki.text\n",
        "    sentences = txt.split(\".\")\n",
        "    k_sentences = []\n",
        "    for sent in sentences:\n",
        "        check_word = sent.split(\" \")\n",
        "        if word in check_word and len(check_word) < 50:\n",
        "            k_sentences.append(sent)\n",
        "        if len(k_sentences) > k:\n",
        "            break\n",
        "    return k_sentences"
      ],
      "metadata": {
        "id": "KNwJQZuPZOOp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenised_additional_corpus(k):\n",
        "    analogy_raw_corpus = []\n",
        "    for analogy in analogies:\n",
        "      if analogy != \" \":\n",
        "          analogy_raw_corpus += get_k_sentences(analogy,k)\n",
        "    full_analogy_corpus = ' '.join([elem for elem in analogy_raw_corpus])\n",
        "    data = re.sub(r'[,!?;-]+\\n:', '.', full_analogy_corpus)\n",
        "    data = nltk.word_tokenize(data)\n",
        "    return data"
      ],
      "metadata": {
        "id": "pX1VdQiaZS8f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_corpus(k):\n",
        "    corpus=tokenised_additional_corpus(k)+corpus_data\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    pattern = re.compile(\"^[a-zA-Z]+$\")\n",
        "    data=[]\n",
        "    for ch in corpus:\n",
        "        ch = ch.lower()\n",
        "        if pattern.match(ch) and ch not in stop_words and ch != '.':\n",
        "                data.append(ch)\n",
        "    return data"
      ],
      "metadata": {
        "id": "6t1Zw7CLZVSf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get full corpus data (after performing pre processing) = gutenburg corpus + k tokenized sentences"
      ],
      "metadata": {
        "id": "6dn5ab-tkzDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_corpus = full_corpus(6) #initailly assuming k=6 to check"
      ],
      "metadata": {
        "id": "-KXt8jtDZYxP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist = nltk.FreqDist(word for word in processed_corpus)\n",
        "print(\"Total words in corpus: \",len(processed_corpus) )\n",
        "print(\"Size of vocabulary: \",len(freq_dist) )\n",
        "print(\"Most frequent tokens: \",freq_dist.most_common(20) )\n",
        "V=len(freq_dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9txhaf6lYcd",
        "outputId": "c06987e8-4072-4a28-d2c4-047f12203630"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in corpus:  1040281\n",
            "Size of vocabulary:  42488\n",
            "Most frequent tokens:  [('shall', 11682), ('said', 9432), ('unto', 9010), ('lord', 8592), ('thou', 6759), ('one', 6291), ('man', 5615), ('thy', 5609), ('god', 5287), ('thee', 4807), ('ye', 4674), ('upon', 4589), ('would', 4054), ('come', 3644), ('could', 3595), ('like', 3470), ('came', 3341), ('day', 3330), ('king', 3158), ('little', 3065)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def indices_info(processed_corpus):\n",
        "    processed = sorted(list(set(processed_corpus)))\n",
        "    index = 0\n",
        "    wordxInd = {}\n",
        "    Indxword = {}\n",
        "    for word in processed:\n",
        "        if word not in wordxInd: \n",
        "             wordxInd[word] = index\n",
        "             Indxword[index] = word\n",
        "             index += 1\n",
        "    return wordxInd, Indxword"
      ],
      "metadata": {
        "id": "Rw6E_WnEvG_V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "wordxInd, Indxword = indices_info(processed_corpus)\n",
        "print(dict(itertools.islice(wordxInd.items(), 5)))\n",
        "print(dict(itertools.islice(Indxword.items(), 5)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_auBB_cErvt1",
        "outputId": "40a5b239-8ff8-4c98-9dc0-a0c614ba4ad2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'aaron': 0, 'aaronites': 1, 'ab': 2, 'aback': 3, 'abaddon': 4}\n",
            "{0: 'aaron', 1: 'aaronites', 2: 'ab', 3: 'aback', 4: 'abaddon'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_windows(words, C):\n",
        "    i = C\n",
        "    while i < len(words) - C:\n",
        "        center_word = words[i]\n",
        "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
        "        yield context_words, center_word\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "M1qzxIIid1GP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in get_windows(processed_corpus[2000:2010],3):\n",
        "    print(f'{x}\\t{y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE-8kVmzeNN3",
        "outputId": "7c5444a6-2c8d-4a45-b5a0-ef37843b51ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['german', 'empire', 'weimar', 'nazi', 'germany', 'berlin']\trepublic\n",
            "['empire', 'weimar', 'republic', 'germany', 'berlin', 'served']\tnazi\n",
            "['weimar', 'republic', 'nazi', 'berlin', 'served', 'scientific']\tgermany\n",
            "['republic', 'nazi', 'germany', 'served', 'scientific', 'artistic']\tberlin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_to_one_hot_vector(word, word2Ind, V):\n",
        "    one_hot_vector = np.zeros(V)\n",
        "    one_hot_vector[word2Ind[word]] = 1  \n",
        "    return one_hot_vector"
      ],
      "metadata": {
        "id": "KJsgoDn511xV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def context_words_to_vector(context_words, word2Ind, V):\n",
        "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
        "    context_words_vectors = np.mean(context_words_vectors, axis=0) \n",
        "    return context_words_vectors"
      ],
      "metadata": {
        "id": "pVUjOeLs1zAk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_example(words, C, word2Ind, V):\n",
        "    for context_words, center_word in get_windows(words, C):\n",
        "        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"
      ],
      "metadata": {
        "id": "dOG19LYZZawG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_set():\n",
        "   train_x =None\n",
        "   train_y = None\n",
        "   for context_words_vector, center_word_vector in get_training_example(processed_corpus, 2, wordxInd, V):\n",
        "       context_words_vector =   np.reshape(context_words_vector, (V, 1))  \n",
        "       center_word_vector   =   np.reshape(center_word_vector, (V, 1))\n",
        "       if train_x is not None:\n",
        "           train_x=np.append(train_x, context_words_vector, axis=1)\n",
        "           train_y=np.append(train_y, center_word_vector, axis=1)\n",
        "       else:\n",
        "           train_x=context_words_vector\n",
        "           train_y=center_word_vector\n",
        "   return train_x,train_y"
      ],
      "metadata": {
        "id": "Qd01-XNL1v13"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(N,V, random_seed=1):\n",
        "    np.random.seed(random_seed)\n",
        "    W1 = np.random.rand(N,V)\n",
        "    W2 = np.random.rand(V,N)\n",
        "    b1 = np.random.rand(N,1)\n",
        "    b2 = np.random.rand(V,1)\n",
        "    return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "0K4TP5McbCRJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    e_z = np.exp(z)\n",
        "    sum_e_z = np.sum(e_z,axis=0,keepdims=True)\n",
        "    return e_z/sum_e_z"
      ],
      "metadata": {
        "id": "EPhYc3QEbCUh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(x, W1, W2, b1, b2):\n",
        "    h = np.dot(W1,x) + b1\n",
        "    h = np.maximum(0,h)\n",
        "    z = np.dot(W2,h) + b2\n",
        "    return z, h"
      ],
      "metadata": {
        "id": "PAiOUrNgbCXy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(y, yhat):\n",
        "    m=y.shape[1]\n",
        "    logprobs = np.multiply(np.log(yhat),y)\n",
        "    cost = - 1/m * np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "R6CS4TIObCbQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def back_prop(x, yhat, y, h, W1, W2, b1, b2):\n",
        "    m=x.shape[1]\n",
        "    l1 = np.dot(W2.T ,(yhat - y))\n",
        "    l1 = np.maximum(0,l1)\n",
        "    grad_W1 = np.dot(l1,x.T)/m\n",
        "    grad_W2 = np.dot((yhat - y),h.T)/m\n",
        "    grad_b1 = np.sum(l1,axis=1,keepdims=True)/m\n",
        "    grad_b2 = np.sum((yhat - y),axis=1,keepdims=True)/m\n",
        "    return grad_W1, grad_W2, grad_b1, grad_b2"
      ],
      "metadata": {
        "id": "GtxsVcpZrV3m"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x, y = get_train_set()"
      ],
      "metadata": {
        "id": "a-frEmT0BEmo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(N, num_iters,alpha, \n",
        "                     random_seed, initialize_model=initialize_model, \n",
        "                     forward_prop=forward_prop, \n",
        "                     softmax=softmax,\n",
        "                     compute_cost=compute_cost, \n",
        "                     back_prop=back_prop):\n",
        "  \n",
        "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=random_seed)\n",
        "    iters = 0\n",
        "    for i in range(num_iters+1):\n",
        "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
        "        yhat = softmax(z)\n",
        "        cost = compute_cost(y, yhat)\n",
        "        if ( (iters+1) % 10 == 0):\n",
        "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2)\n",
        "\n",
        "        W1 = W1 - alpha * grad_W1\n",
        "        W2 = W2 - alpha * grad_W2\n",
        "        b1 = b1 - alpha * grad_b1\n",
        "        b2 = b2 - alpha * grad_b2\n",
        "        iters +=1 \n",
        "        if iters == num_iters: \n",
        "            break\n",
        "            \n",
        "    return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "HrtBz1uy6RRf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AC2EfcwG6jbL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}