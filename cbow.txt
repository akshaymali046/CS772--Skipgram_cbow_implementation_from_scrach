import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def softmax(x):
    exp_x = np.exp(x)
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

class CBOW:
    def __init__(self, context_size, hidden_size, word_size):
        self.context_size = context_size
        self.hidden_size = hidden_size
        self.word_size = word_size

        # Initialize weights
        self.W1 = np.random.randn(context_size * 2, hidden_size)
        self.W2 = np.random.randn(hidden_size, word_size)

    def forward(self, X):
        h = np.dot(X, self.W1)
        h = sigmoid(h)
        y_pred = np.dot(h, self.W2)
        y_pred = softmax(y_pred)
        return y_pred

    def backward(self, X, y_true, y_pred, learning_rate):
        # Compute loss
        loss = -np.mean(y_true * np.log(y_pred))

        # Compute gradients
        dW2 = np.dot(y_pred - y_true, y_pred.T)
        dh = np.dot(y_pred - y_true, self.W2.T) * sigmoid_derivative(y_pred)
        dW1 = np.dot(X.T, dh)

        # Update weights
        self.W1 -= learning_rate * dW1
        self.W2 -= learning_rate * dW2

        return loss


# Define context size, hidden layer size, and vocabulary size
context_size = 2
hidden_size = 4
word_size = 5

# Initialize CBOW model
cbow = CBOW(context_size, hidden_size, word_size)

# Generate a random input example
X = np.random.randn(1, context_size * 2)

# Generate a one-hot encoded target label
y_true = np.zeros((1, word_size))
y_true[0, np.random.randint(0, word_size)] = 1

# Perform forward pass
y_pred = cbow.forward(X)

# Perform backward pass
learning_rate = 0.1
loss = cbow.backward(X, y_true, y_pred, learning_rate)
